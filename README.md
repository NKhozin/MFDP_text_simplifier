## Проект по курсу My First Data Project 2 от ODS.ai: сервис по суммаризации текстов
### Введение
В современном информационном мире мы сталкиваемся с непрерывным потоком текстовых данных, которые требуют быстрой и эффективной обработки. Редакторы, журналисты и специалисты по социальным медиа сталкиваются с постоянной необходимостью кратко и емко представить информацию, сохранив при этом исходный смысл. Однако, обработка больших объемов текста и поиск наиболее значимых сведений может быть утомительной и времязатратной задачей. Исходя из этой проблемы появилась идея создания сервиса по автоматическому упрощению текстовых данных.
### Цель
Создание сервиса для автоматической суммаризации текста.
### Задачи
- Поиск подходящих корпусов параллельных текстов и их обработка
- Выбор и обучение модели, подбор гиперпараметров
- Создание интерфейса для взаимодействия с пользователями
### Данные для обучения

Данные были предварительн очищены. Для всех корпусов оставлены только те пары:
- где целевой текст больше по количеству токенов, чем исходный
- где в исходном тексте 10 и более токенов, а в целевом не менее 5
- где целевой текст не является подстрокой исходного
- для корпуса перефразирования уберем тексты, где в целевом содержится менее 20 и более 80 процентов от исходного текста


| Название датасета | Количество | Макс. длина исходного предложения (токенов) | Средняя длина исходного предложения (токенов) | Средняя длина целевого предложения (токенов)
|-------|-------|-------|-------|-------|
| gazeta | 63434 | 2841 | 1353 | 91 |
| para_phraser | 1143158 | 5663 | 23 | 16 |
| ru_adapt | 41563 | 8180 | 68 | 37 |
| ru_simp | 6432 | 168 | 42 | 26 |
| ru_xlsum | 66669 | 37904 | 1173 | 54 |

По данным корпусам были рассчитаны различные метрики сходства предложений внутри корпусов: косинусные близости, BLEU, доля общих символьных ngram, средняя перплексия предложений в корпусе.
| Название датасета | sim | sim_random | bleu_1 | bleu_2 | bleu_ | char_ngram_overlap | perp_1 | perp_2 | perp_mean |
|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| ru_xlsum | 0.54 | 0.27 | 0.01 | 0.08 | 0.05 | 0.08 | 3.03 | 3.06 | 1.65 |
| ru_adapt | 0.74 | 0.22 | 0.52 | 0.58 | 0.55 | 0.52 | 4.05 | 3.70 | 3.07 |
| gazeta | 0.68 | 0.26 | 0.0002 | 0.07 | 0.04 | 0.09 | 2.99 | 3.13 | 1.61 |
| ru_simp | 0.77 | 0.27 | 0.36 | 0.42 | 0.39 | 0.34 | 4.23 | 4.12 | 3.34 |
| para_phraser | 0.66 | 0.32 | 0.32 | 0.34 | 0.27 | 4.65 | 4.09 | 3.52 | 1.65 |
***
## Методология
В качестве моделей, которые будут дообучаться на задачу суммаризации текста, будут использоваться:
- T5
- GPT-3
- BART
Метрики:
- ROUGE
- SARI

***
### Метрики
В таблице ниже можно увидеть значения метрик на тестовом наборе данных при различных комбинациях данных для обучения: изменялось , 
![image](https://github.com/NKhozin/mfdp_text_simplifier/assets/92330362/ffec12d2-a7f3-4792-a502-740c80a3a47d)
***
### Пример работы модели rut5_v8 в телеграмм боте
Пример предсказания модели на данных, которых не было в обучающем наборе, из новостных каналов. 
![image](https://github.com/NKhozin/mfdp_text_simplifier/assets/92330362/9f6d55b5-16a3-488f-addd-f305e84f20e3)
***
### Описание файлов репозитория:
- calculate_metrics	- расчеты метрик на тестовом наборе.
- data_prepare_main - подготовка данных, разделение их на тренировочный, валидационный и тестовый наборы, очистка данных.
- fit_model - обучение моделей.
- inference_example - пример использования модели.
- ru_t5_optimize - оптимизация параметров модели T5.
- tg_bot - код для запуска телеграмм бота.
***
## Инструкция по использованию
### Модель: https://huggingface.co/nikitakhozin/t5_summarization
### Контейнер телеграмм бота: https://hub.docker.com/r/nkhozin/summarization_bot
### Контейнер с использованием моего телеграмм бота:
1) Скачать образ контейнера с телеграмм ботом: <br />
```docker pull nkhozin/summarization_bot```
2) Развернуть контейнер: <br />
```docker run nkhozin/summarization_bot```
3) Открыть в телеграмме бота - https://t.me/text_simplifier_bot
4) Подавать текст для суммаризации с префиксом:
"/simplify <your_text>"

### Использование модели в своем телеграмм боте:
1) Скачать mfdp_text_simplifier/tg_bot.
2) В main_bot.py подставить на место <YOUR_TG_BOT_TOKEN> свой токен от телеграмм бота.
3) Установить необходимые версии библиотек из файла requirements.txt: <br />
```pip install -r requirements.txt```
5) Запустить в main_bot.py: <br />
```bot.polling()```
5) Зайти в свой телеграмм бот. Подавать текст для суммаризации с префиксом:
"/simplify <your_text>"

### Использование модели в Python:
1) Скачать mfdp_text_simplifier/inference_example.
2) Установить необходимые версии библиотек из файла requirements.txt: <br />
```pip install -r requirements.txt```
4) Запустить функцию generate_t5(text) в rut5_inference.ipynb, где text - исходный текст для суммаризации.
***
Основной модуль приложения main_bot.py состоит из двух частей:
1) Функция генерации сокращенного варианта исходного текста - generate_t5.
2) Телеграмм бот с функциями отправки приветственного текста в начале взаимодействия с ним после команды "/start", а также обработкой команды "/simplify <text>", где text - исходный текст для суммаризации из сообщения.
